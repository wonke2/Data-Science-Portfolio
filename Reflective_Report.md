# COMP2200 Reflective Report
## Introduction
This unit is an introduction to the fundamentals of data science, and the tools and techniques used to analyse data, such as displaying data graphically. Covering the basics of data analysis, including data visualisation, data wrangling, and data modelling, COMP2200 introduced us to displaying data graphically, using predictive models, evaluating methods and more. Throughout COMP2200 we also went through the basics of machine learning, including the evaluation of machine learning models.

Data visualization was a key concept we covered, enabling us to present information in visually appealing and intuitive ways, allowing us to easily understand and interpret patterns, trends, and relationships within the data. 

In addition to data visualization, we delved into the crucial process of data wrangling, transforming and cleaning raw data to prepare it for analysis. We learned how to handle missing values, deal with outliers, and pre-process data for further exploration. Moreover, the unit introduced us to the intricacies of data modelling, equipping us with the skills to build meaningful models and representations that capture the essence of the underlying data. Through the lens of graphical representation, we were able to grasp the dynamics of data patterns and make informed predictions using predictive models.

Furthermore, COMP2200 provided us with a solid foundation in the fundamentals of machine learning. Through exploring evaluation techniques such as MSE, r2 and cross-validation, we are able to evaluate machine learning model performance. During this unit we applied these techniques to data-sets related to real-world scenarios, giving us a deeper understanding of the strengths and limitations of different machine learning algorithms and their applications in real life.

## First Starting to Use ipynb
Using Jupyter Notebook through Anaconda3 was confusing for the first time due to the lack of friendly UI and customisation. The requirement to manually save each change and delete the autogenerated .checkpoint files were fairly annoying so I opted to use Visual Studio Code. After installing the python extension, I was able to use the same key features in Anacondas web notebook, but with the added benefit of being able to save automatically and not having to delete the .checkpoint files. The only requirement was to set the kernel to the one installed by anaconda which is simple and easy to do. With the more appealing UI and benefits of VSCode such as Source Control, I was able to work more efficiently and effectively than while using the local server through Anaconda. Along with changing to VSC i was not very confident in using python, knowing only the basic syntax. However, through the continued use of it, I’m much more confident in my ability to use python.

## Problem-Solving Process
When we first imported raw data, in order to prepare it for analysis we had to clean it and ensure it is suitable to be used. During part 4 of this portfolio, I had an issue of too many irrelevant columns. In order to decide which to keep, I graphed the data and noticed that some columns clearly had no correlation to the data. I then decided to remove these columns as they were not needed. Some columns were also just a subset of another column, so I removed those as well. After selecting the relevant columns, I had to clear any rows with null values as they would cause issues with the analysis further down the line, I simultaneously removed null and outliers using the code;
``` python
    for column in data.select_dtypes(exclude='object'):
        z_scores = np.abs((data[column] - data[column].mean()) / data[column].std())
        outliers = data[z_scores > 3][column]
        data[column][outliers.index] = np.nan 
    data = data.dropna()
```
We remove outliers in order to ensure that when we train the model it won’t be skewed by them.

Even after only removing clearly uncorrelated features we still must select features that have a positive correlation to the target feature. We use correlation maps first to show how each feature correlates with each other, and we filter for the positively correlated feature. I did this using the code;
``` python
#Assuming that seaborn has been imported as sns and matplotlib.pyplot has been imported as plt
    #show correlation map
    corr_matrix = data.corr()
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
    plt.show()

    # List of features with a correlation to W_HOME of 0.2 and above
    corr_matrix = data.corr()['W_HOME']
    relevant_features = corr_matrix[(np.abs(corr_matrix) > 0.2)].index
    relevant_features
```
Another common issue to look out for while preparing data for analysis is if there are large data discrepancies. We check for this through graphing the data and describing it in python. I.e.:
``` python
    #describe data
    data.describe().transpose()

    #graph data
    data.hist(bins=10 ,figsize=(16,12), color = 'Green')
    plt.show()
 ```
If there are we need to normalise data or while training our model of choice we will have scaling issues due to differing scales and ranges. features with a larger scale will seem more significant to the model, skewing the result. We fix this by using ` Standard Scaler()` from `sklearn.preprocessing`. I did this using the code;
``` python
    #Assuming that StandardScaler has been imported
    scaler = StandardScaler()
    standardized_x_train = scaler.fit_transform(x_ex1_train)
    standardized_x_test = scaler.transform(x_ex1_test)
```

# What I've Learnt
## Data Structures
### NumPy
The main NumPy data structures are:
* ndarray 
    * A multidimensional array object
* Scalars
    * Single value types
    * can be used in combination with other NumPy data structures or as standalone variables
* Structured Arrays
    * Arrays with compound data types
* Masked Arrays
    * Arrays with missing or invalid entries
* Matrices
    * Specialised 2D arrays
    * Added linear algebra functionality

### Pandas
The main Pandas data structures are:
* Series
    * 1D labelled array
    * Can hold data of different types
* DataFrame
    * 2D labelled data structure
    * Consists of rows and columns, where each column can contain data of different types.
    * Similar to a dictionary of Series objects
* Index
    * Holds labels or names for the rows or columns in a DataFrame
    * Provides a way to reference and access data
    * Immutable

## Different models
### Linear Regression
* Models the relationship between a dependent variable and one or more independent variables using a linear equation
* Aims to find the best-fitting line that minimizes the difference between observed and predicted values
    * Lead to prediction and inference based on resulting linear equation 

### Logistic Regression
* Used for binary classification
* Categorical outcome
* Estimates the relationship between independent variables and logarithm of the probablities of each outcome
* Handles linear and nonlinear relationships
### And More
Including:
* Naive Bayes (NB) Classifier
* K-Nearest Neighbours (KNN) Classifier
* Decision Trees

# Summary
COMP2200 provided the basics to Data Science, covering techniques and concepts such as data visualization, data wrangling, and data modeling. Through these techniques we can analyse and interpret data correctly to identify relationships. We also covered the fundamentals of machine learning, where, through evaluating techniques, we can judge how a model performs. We also learnt the strengths and limitations of different machine learning algorithms. COMP220 has given me a solid foundation in the basics of data science and machine learning, and I look forward to applying these skills in future units and projects.